{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n\ndata = pd.read_csv('../input/letterrrrrrr/letterscg.txt',sep=' ')\n# data.describe(include='all')\nonlydata=data.loc[:,'x-box':'yegvx']\nonlydata.describe(include='all')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-08T16:25:40.079210Z","iopub.execute_input":"2022-04-08T16:25:40.079576Z","iopub.status.idle":"2022-04-08T16:25:40.145546Z","shell.execute_reply.started":"2022-04-08T16:25:40.079537Z","shell.execute_reply":"2022-04-08T16:25:40.144508Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nX=onlydata.values\nyCG=data.values[:,0]\ny=np.zeros((yCG.size))\ny[yCG=='G']=1\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 0)\n# Standardize the x_train and x_test datasets\nstd_scaler = preprocessing.StandardScaler().fit(X_train)\nX_train_scaled = std_scaler.transform(X_train)\nX_test_scaled = std_scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T16:25:58.441492Z","iopub.execute_input":"2022-04-08T16:25:58.441782Z","iopub.status.idle":"2022-04-08T16:25:58.453379Z","shell.execute_reply.started":"2022-04-08T16:25:58.441753Z","shell.execute_reply":"2022-04-08T16:25:58.452274Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\npca= PCA(n_components=2)\nfig, (plttrain, plttest) = plt.subplots(1, 2)\ndatavl1=pca.fit_transform(X_train_scaled)\ng11=datavl1[(y_train==1)]\ng12=datavl1[(y_train==0)]\nplttrain.set_title('Train values :'+ str(X_train_scaled.shape))\nplttrain.scatter(g11[:,0],g11[:,1],marker='o',c=\"blue\")\nplttrain.scatter(g12[:,0],g12[:,1],marker='+',c=\"red\")\nplttrain.legend(['lable 1','lable 0'])\ndatavl2=pca.fit_transform(X_test_scaled)\ng21=datavl2[(y_test==1)]\ng22=datavl2[(y_test==0)]\nplttest.set_title('Test values :'+str(X_test_scaled.shape))\nplttest.scatter(g21[:,0],g21[:,1],marker='o',c=\"black\")\nplttest.scatter(g22[:,0],g22[:,1],marker='+',c=\"yellow\")\nplttest.legend(['lable 1','lable 0'])\nplt.savefig('lettersCG_Xtraintext.png')","metadata":{"execution":{"iopub.status.busy":"2022-04-08T16:26:10.924219Z","iopub.execute_input":"2022-04-08T16:26:10.924763Z","iopub.status.idle":"2022-04-08T16:26:11.745634Z","shell.execute_reply.started":"2022-04-08T16:26:10.924719Z","shell.execute_reply":"2022-04-08T16:26:11.744467Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\npca= PCA(n_components=2)\n# fig, (plttrain, plttest) = plt.subplots(1, 2)\ndatavlx=pca.fit_transform(X)\ngx1=datavlx[(y==1)]\ngx2=datavlx[(y==0)]\nplt.title('Train values :'+ str(X.shape))\nplt.scatter(gx1[:,0],gx1[:,1],marker='o',c=\"gray\")\nplt.scatter(gx2[:,0],gx2[:,1],marker='+',c=\"blue\")\nplt.legend(['lable 1','lable 0'])\nplt.savefig('lettersCG_X.png')","metadata":{"execution":{"iopub.status.busy":"2022-04-08T16:26:21.362784Z","iopub.execute_input":"2022-04-08T16:26:21.363974Z","iopub.status.idle":"2022-04-08T16:26:21.812147Z","shell.execute_reply.started":"2022-04-08T16:26:21.363884Z","shell.execute_reply":"2022-04-08T16:26:21.811194Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class RatingModel:\n    def __init__(self, y_, y_Pr):\n      self.y_=y_\n      self.y_Pr=y_Pr\n      self.TN=np.size(y_Pr[(y_Pr==-1)&(y_==y_Pr)])\n      self.FN=np.size(y_Pr[(y_Pr==-1)&(y_!=y_Pr)])\n      self.TP=np.size(y_Pr[(y_Pr==1)&(y_==y_Pr)])\n      self.FP=np.size(y_Pr[(y_Pr==1)&(y_!=y_Pr)])\n      self.y_[self.y_==0]=-1\n      self.y_Pr[self.y_Pr==0]=-1\n      # assert self.y_.set={1, -1}\n      # assert self.y_Pr.set={1, -1}\n    def __rep__():\n        return \"\"\n    def accur_Error(self, y_, y_Pr):\n        rs=(self.TP+self.TN)/(y_.size)\n        return [rs,(1-rs)]\n    def sensitivity(self):\n        P=np.size(self.y_[self.y_==1])\n        return (self.TP)/(P)\n    def specificity(self):\n        N=np.size(self.y_[self.y_==-1])\n        return (self.TN)/(N)\n    def precision(self):\n        rs=self.TP+self.FP\n        return (self.TP)/(rs)\n    def recall(self):\n        rs=self.TP+self.FN\n        return (self.TP)/(rs)\n    def rating(self):\n        return [self.accur_Error(self.y_, self.y_Pr), self.sensitivity(), self.specificity(), self.precision(), self.recall()]\nclass DecisionStump:\n    def __init__(self, T=100):\n        self.T = T\n        pass\n\n    def fit(self, X: np.ndarray, y: np.ndarray, sample_weight: np.ndarray):\n        T = self.T\n        W=sample_weight\n        nrow, ncol = X.shape\n        assert nrow == y.size\n\n        bestn = 0\n        bestd = 1\n        bestp = 0\n        minerr = W.sum()\n        for i in range(ncol):\n            err, d, p = self._optimize(X[:, i], y, W, T)\n            if err < minerr:\n                minerr = err\n                bestn = i\n                bestd = d\n                bestp = p\n        \n        self.features = ncol\n        self.bestn = bestn\n        self.bestd = bestd\n        self.bestp = bestp\n\n        return self\n\n    def _optimize(self, X, y, W, T):\n        X = X.flatten()\n        min_x, max_x = X.min(), X.max()\n        len_x = max_x - min_x\n        \n        bestd = 1\n        bestp = min_x\n        minerr = W.sum()\n\n        if len_x > 0.0:\n            for p in np.arange(min_x, max_x, len_x/T):\n                for d in [-1, 1]:\n                    gy = np.ones((y.size))\n                    gy[X*d < p*d] = -1\n                    err = np.sum((gy != y)*W)\n                    if err < minerr:\n                        minerr = err\n                        bestd = d\n                        bestp = p\n\n        return minerr, bestd, bestp\n\n    def predict(self, test_set : np.ndarray):\n        nrow, ncol = test_set.shape\n\n        assert ncol == self.features\n\n        icol = test_set[:, self.bestn]\n        h = np.ones((nrow))\n        h[icol*self.bestd < self.bestp*self.bestd] = -1\n        return h\nclass AdaBoost:\n    def __init__(self , T, hmodel = DecisionStump()):\n        self.T=T\n        self.hmodel=hmodel\n    def fit(self, X: np.ndarray, y_: np.ndarray, verbose=False):\n      n = X.shape[0]\n      T = self.T\n      y=y_\n      y[y==0]=-1\n    # init numpy arrays\n      self.D = np.zeros(shape=(T, n))\n      self.h = np.zeros(shape=T, dtype=object)\n      self.alpha = np.zeros(shape=T)\n      self.errors = np.zeros(shape=T)\n      self.ratting = np.zeros(shape=(T,2))\n\n      # initialize weights uniformly\n      self.D[0] = np.ones(shape=n) / n\n\n      for t in range(T):\n          # fit  weak learner\n          D_ = self.D[t]\n          h_ = DecisionStump(60)\n          h_ = h_.fit(X, y, D_)\n\n          # calculate error and stump weight from weak learner prediction\n          Pr_ = h_.predict(X)\n          error_ = D_[(Pr_ != y)].sum()# / n\n          alpha_ = np.log((1 - error_) / error_) / 2\n\n          # update sample weights\n          D_new = (\n              D_ * np.exp(-alpha_ * y * Pr_)\n          )\n          \n          D_new /= D_new.sum()\n\n          # If not final iteration, update sample weights for t+1\n          if t+1 < T:\n              self.D[t+1] = D_new\n\n          # save results of iteration\n          self.h[t] = h_\n          self.alpha[t] = alpha_\n          self.errors[t] = error_\n          # ae=np.array([0,0])\n          if t>0:\n            Pr_temp=self.predictmodul(X,t)\n            modelra=RatingModel(y, Pr_temp)\n            self.ratting[t,:]=modelra.accur_Error(y, Pr_temp)\n          if verbose: print('TruongThanhTrieu {0}-th weak TruongThanhTrieu: accuracy={1}, error={2}'.format (t, self.ratting[t,0], self.ratting[t,1]))\n      return self\n    def predict(self, X):\n        Pr_ = np.array([h_.predict(X) for h_ in self.h])\n        return np.sign(np.dot(self.alpha, Pr_))\n    def predictmodul(self, X, i):\n        h_temp=self.h[:i]\n        alpha_temp=self.alpha[:i]\n        Pr_ = np.array([h_.predict(X) for h_ in h_temp])\n        return np.sign(np.dot(alpha_temp, Pr_))","metadata":{"execution":{"iopub.status.busy":"2022-04-08T16:27:09.210498Z","iopub.execute_input":"2022-04-08T16:27:09.210821Z","iopub.status.idle":"2022-04-08T16:27:09.252093Z","shell.execute_reply.started":"2022-04-08T16:27:09.210789Z","shell.execute_reply":"2022-04-08T16:27:09.250549Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model=AdaBoost(60)\nmodel=model.fit(X_train_scaled, y_train,  True )\nPr=model.predict( X_test_scaled)\nPr[(Pr==0)]=-1\n# print(Pr, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-08T16:27:14.535135Z","iopub.execute_input":"2022-04-08T16:27:14.535789Z","iopub.status.idle":"2022-04-08T16:27:20.317532Z","shell.execute_reply.started":"2022-04-08T16:27:14.535736Z","shell.execute_reply":"2022-04-08T16:27:20.316559Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"ra_Xtest = np.zeros(shape=(model.T,2))\nfor i in range(1,model.T):\n  Pr_i=model.predictmodul(X_test_scaled,i)\n  modelra=RatingModel(y_test, Pr_i)\n  ra_Xtest[i,:]=modelra.accur_Error(y_test, Pr_i)\nra_Xtrain = np.zeros(shape=(model.T,2))\nfor i in range(1,model.T):\n  Pr_i=model.predictmodul(X_train_scaled,i)\n  modelra=RatingModel(y_train, Pr_i)\n  ra_Xtrain[i,:]=modelra.accur_Error(y_train, Pr_i)\niter=range(model.T)\nplt.plot(iter,ra_Xtest[:,0],'y-', label='Test accuracy')\nplt.plot(iter,ra_Xtest[:,1],'r-', label='Test error')\nplt.plot(iter,ra_Xtrain[:,0],'y--', label='Train accuracy')\nplt.plot(iter,ra_Xtrain[:,1],'r--', label='Train error')\nplt.legend(loc='center right')\nplt.xlabel('Iter')\nplt.ylabel('Loss/Accuracy')\nplt.savefig('lettersCG_model.png')","metadata":{"execution":{"iopub.status.busy":"2022-04-08T16:27:30.513322Z","iopub.execute_input":"2022-04-08T16:27:30.513617Z","iopub.status.idle":"2022-04-08T16:27:30.992160Z","shell.execute_reply.started":"2022-04-08T16:27:30.513587Z","shell.execute_reply":"2022-04-08T16:27:30.991301Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"sumerror=0;\ny_new=y_test\ny_new[y_new==0]=-1\n# for i in range(y_new.shape[0]):\n#   if y_new[i]!=Pr[i]: \n#     sumerror+=1\nsumerror=np.size(y_new[Pr!=y_new])\ngT1=datavl2[(Pr==1)]\ngT0=datavl2[(Pr==-1)]\ngF1=datavl2[(y_new!=Pr)&(Pr==1)]\ngF0=datavl2[(y_new!=Pr)&(Pr==-1)]\nplt.title('Test values errors :'+str(sumerror)+'/ '+str(X_test_scaled.shape[0]))\nplt.scatter(gT1[:,0],gT1[:,1], marker='o')\nplt.scatter(gT0[:,0],gT0[:,1], marker='+')\nplt.scatter(gF1[:,0],gF1[:,1], c=\"pink\", marker='o')\nplt.scatter(gF0[:,0],gF0[:,1], c=\"pink\", marker='+')\nplt.legend(['Predict true 1', 'Predict true 0', 'Predict false'])\nplt.savefig('lettersCG_XtextError.png')","metadata":{"execution":{"iopub.status.busy":"2022-04-08T16:27:40.930088Z","iopub.execute_input":"2022-04-08T16:27:40.930708Z","iopub.status.idle":"2022-04-08T16:27:41.301893Z","shell.execute_reply.started":"2022-04-08T16:27:40.930625Z","shell.execute_reply":"2022-04-08T16:27:41.300870Z"},"trusted":true},"execution_count":15,"outputs":[]}]}